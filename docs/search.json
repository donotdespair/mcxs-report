[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estimating the impact of monetary policy shocks on different housing indicators: A Bayesian SVAR Approach",
    "section": "",
    "text": "Abstract. This project uses a Bayesian SVAR approach to estimate the effects of domestic and foreign monetary policy shocks on housing prices and number of new houses in Australia. The identification relies on imposing exclusion-restrictions and the estimation process follows the D. F. Waggoner and Zha (2003) algorithm using the Gibbs sampler. In the extended version, we estimate the hyperparameters of the priors of the model. We find that a positive domestic monetary policy shock reduces both the number of new houses and housing prices, while a positive foreign (US) monetary policy shock reduces the number of new houses but increases housing prices.\nKeywords. bsvars, impulse responses, quarto, R, housing price index, monetary policy shocks"
  },
  {
    "objectID": "index.html#empirical-project-setup",
    "href": "index.html#empirical-project-setup",
    "title": "Estimating the impact of monetary policy shocks on different housing indicators: A Bayesian SVAR Approach",
    "section": "Empirical Project Setup",
    "text": "Empirical Project Setup\nThis project website is being developed as a Quarto document and the empirical work in conducted using R. The necessary datasets are imported from the Reserve Bank of Australia (RBA) and the Australian Bureau of Statistics (ABS) websites using readrba and readabs respectively."
  },
  {
    "objectID": "index.html#choice-of-variables",
    "href": "index.html#choice-of-variables",
    "title": "Estimating the impact of monetary policy shocks on different housing indicators: A Bayesian SVAR Approach",
    "section": "Choice of variables",
    "text": "Choice of variables\nI use the following variables to answer this question. I discuss the relevance of each variable.\n\n\\(\\log(M1)\\): represents the log of the money supply M1. Both conventional and unconventional monetary policy shocks can change the stock of money supply and its size affects real variables of the economy.\n\\(\\Delta CPI\\): represents the year-on-year change in the Consumer Price Index (CPI). It is a measure of inflation in an economy and is affected by monetary policy shocks.\n\\(\\log(c)\\): represents the log of consumption of the economy. Monetary policy shocks can alter people’s consumption-savings behaviour.\n\\(\\log(GDP)\\): represents the log of the Gross Domestic Product (GDP). Including this along with the consumption helps differentiate the effect on the non-consumption aspect of the economy.\n\\(loanrate\\): represents the weighted average interest rates on owner-occupied home loans.\n\\(AUCR\\): represents the Australian Cash Rate Target. This is the major monetary policy instrument available to the RBA.\n\\(USFFR\\): represents the Federal Funds Rate Maximum Target Rate. Monetary policy adopted in the US tend to ripple into other economies so this is a variable of interest. Another extension to this variable would be to include the Target rates of Australia’s largest trading partners.\n\\(nhouses\\): represents the number of new private dwellings (houses) approved for construction in Australia. Impact on housing prices might be dampened by the supply elasticity of housing captured by this variable.\n\\(PPI\\): represents the Property Price Index in Australia. The index is normalized with respect to the property prices in 2011-2012."
  },
  {
    "objectID": "index.html#data-properties",
    "href": "index.html#data-properties",
    "title": "Estimating the impact of monetary policy shocks on different housing indicators: A Bayesian SVAR Approach",
    "section": "Data Properties",
    "text": "Data Properties\nThe variables discussed above are illustrated in the figure below. Note that the logged variables trend upwards because they are expressed in their levels, while variables expressed in percentage change terms do not exhibit this behaviour.\nThe dataset will be used in the sample consists quarterly data from 2003 Q3 to 2021 Q4, including 74 observations, plots of truncated dataset is shown as:\n\nvar_names <- colnames(df)\ndate_range <- as.yearqtr(rownames(df), format = \"%Y Q%q\")\npar(mfrow = c(3,3), mar=c(2,2,2,2))\nfor (j in 1:ncol(df)){\n  plot(x = date_range, y = df[,j],,type='l',\n       main = paste(var_names[j]), ylab = \"\", xlab = \"\",\n       lwd = 2.5,\n       ylim = c(min(df[,j]),max(df[,j])))\n}\n\n\n\n\n\n\n\n\nADF Tests\nNext, I perform and display ADF test results on the variables. I report, for each variable, the difference level at which the ADF tests rejects the null that the series is non-stationary.\n\nperform_adf_tests <- function(df) {\n  # Create an empty dataframe to store the results\n  results <- data.frame(Variable = character(), TestType = character(), \n                        TestStatistic = numeric(), PValue = numeric(), \n                        stringsAsFactors = FALSE)\n\n  # Iterate over each column in the dataframe\n  for (col in colnames(df)) {\n    # Remove NA values from the column\n    column_data <- na.omit(df[[col]])\n\n    # Perform ADF test for levels\n    adf_levels <- tseries::adf.test(na.omit(column_data), k = 4)\n\n    # Check if p-value is less than or equal to 0.05\n    if (adf_levels$p.value <= 0.05) {\n      results <- bind_rows(results,\n        data.frame(Variable = col, TestType = \"Levels\", \n                   TestStatistic = adf_levels$statistic,\n                   PValue = adf_levels$p.value)\n      )\n    } else {\n      # Perform ADF test for first difference\n      adf_diff1 <- tseries::adf.test(na.omit(diff(column_data)), k = 4)\n      \n      # Check if p-value is less than 0.05\n      if (adf_diff1$p.value < 0.05) {\n        results <- bind_rows(results,\n          data.frame(Variable = col, TestType = \"First Difference\", \n                     TestStatistic = adf_diff1$statistic,\n                     PValue = adf_diff1$p.value)\n        )\n      } else {\n        # Perform ADF test for second difference\n        adf_diff2 <- tseries::adf.test(na.omit(diff(column_data, differences = 2)), k = 4)\n        \n        results <- bind_rows(results,\n            data.frame(Variable = col, TestType = \"Second Difference\", \n                       TestStatistic = adf_diff2$statistic,\n                       PValue = adf_diff2$p.value)\n          )\n      }\n    }\n  }\n\n\n  # Return the results dataframe\n  return(results)\n}\n\nadf_test_results <- perform_adf_tests(df)\nrmarkdown::paged_table(adf_test_results)\n\n\n\n  \n\n\n\n\n\n\n\n\nACF Plots\n\n\n\n\n\nACF Plots for our data\n\n\n\n\n\n\n\n\n\nPACF Plots\n\n\n\n\n\nPACF Plots for our data"
  },
  {
    "objectID": "index.html#identification",
    "href": "index.html#identification",
    "title": "Estimating the impact of monetary policy shocks on different housing indicators: A Bayesian SVAR Approach",
    "section": "Identification",
    "text": "Identification\nI plan to use exclusion-restrictions to identify the structural matrix \\(B_0\\). In particular, I will impose a lower-triangular restriction on \\(B_0\\) and employ the solution concept in D. Waggoner and Zha (2003) who use a normalization rule as an optimal solution to the local identification problem. I will then employ the Gibbs sampler for a SVAR model with exclusion restrictions as in D. F. Waggoner and Zha (2003) to obtain draws for \\(B_0\\) and \\(B_+\\).\n\\[ B_0 Y = B_+ X + U, \\qquad \\qquad U|X \\sim \\mathcal{MN}_{N \\times T}(\\textbf{0}_{N \\times T}, I_T, I_N) \\] where\n\\(B_0\\) is a \\(N \\times N\\) contemporaneous effects matrix.\n\\(Y = [y_1, \\dots, y_T]\\) is a \\(N \\times T\\) matrix of observations.\n\\(B_+ = [B_d, B_1, \\dots, B_p]\\) is a \\(N \\times K\\) matrix of autoregressive parameters, where \\(K = Np + d\\) (\\(d\\) is the number of deterministic terms; \\(p\\) is the number of lags).\n\\(X = [x_1, \\dots, x_T]\\) is a \\(K \\times T\\) matrix of lagged observations where each \\(x_t = (1, y_{t-1}, \\dots, y_{t-p})'\\).\n\\(U = [u_1, \\dots, u_T]\\) is a \\(N \\times T\\) matrix of structural shocks.\nFor convenience of coding and inference purposes, we consider a row-wise equation form as follows: \\[ B_{0[n.\\cdot]} Y = B_{+n} X + U_n, \\qquad \\qquad U_n|X \\sim \\mathcal{N}(\\mathbf{0}_T, I_T) \\]\nIf \\(r_n\\) denotes the number of elements in the \\(n^{th}\\) row of \\(B_0\\) that stay unrestricted, then we can further decompose \\(B_{0[n.\\cdot]}\\) into \\(b_n\\) and \\(V_n\\).\n\\(b_n\\) is a \\(1 \\times r_n\\) vector of unrestricted elements in the \\(n^{th}\\) row of \\(B_0\\).\n\\(V_n\\) is a \\(r_n \\times N\\) matrix which places elements of \\(b_n\\) in the appropriate positions to impose the restrictions on \\(B_0\\).\nThen, the row-wise equation form can be written as follows: \\[ b_n V_n Y = B_{+n} X + U_n, \\qquad \\qquad U_n|X \\sim \\mathcal{N}(\\mathbf{0}_T, I_T) \\] Following D. F. Waggoner and Zha (2003) and Arias, Rubio‐Ramírez, and Waggoner (2018), we define that \\((B_+, B_0)\\) follow jointly a Normal-Generalised Normal (NGN) distribution denoted as \\[ p(B_+, B_0) \\sim \\mathcal{NGN}(B, \\Omega, S, \\nu)\\]\nif \\(B_{+n}\\) follows a K-variate normal distribution given \\(b_n\\) \\[ p(B_{+n}|b_n) = \\mathcal{N}_k(b_nV_nB, \\Omega) \\] with kernel \\[ p(B_{+n}|b_n) \\propto \\exp \\left\\{  -\\frac{1}{2} \\left( B_{+n} - b_nV_nB \\right) \\Omega^{-1}  \\left( B_{+n} - b_nV_nB \\right)' \\right\\} \\]\nfor \\(n = 1, \\dots, N\\) and \\(b_1, \\dots, b_N\\) jointly have a distribution whose kernel is specified by \\[ p(b_1, \\dots, b_N) \\propto | \\det(B_0) |^{\\nu - N} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N b_n V_n S^{-1} V_n' b_n'  \\right\\} \\]\nThe joint-distribution of \\((B_+, B_0)\\) can then be written as \\[ p(B_0, B_+) = \\left( \\prod_{n=1}^N p(B_{+n}|b_n)\\right) p(b_1, \\dots, b_N) \\]\nThis classification of the joint-distribution allows us to obtain natural-conjugate prior and corresponding posterior distributions.\nMoreover, the following function imposes a lower-triangular restriction on \\(B_0\\) and creates a list containing a matrix of \\(b_n\\) and corresponding \\(V_n\\) row vectors.\n\nltexclusion = function(usedata){\n  BM.V        = vector(\"list\",usedata$N)\nfor (n in 1:usedata$N){\n  BM.V[[n]]   = cbind(diag(n),matrix(0,n,usedata$N-n))\n}\n\nB0.initial    = matrix(0,usedata$N,usedata$N)\nfor (n in 1:usedata$N){\n  unrestricted               = apply(BM.V[[n]],2,sum)==1\n  B0.initial[n,unrestricted] = rnorm(sum(unrestricted))\n}\nB0Vlist       = list(B0.initial = B0.initial, V = BM.V)\n}"
  },
  {
    "objectID": "index.html#basic-model",
    "href": "index.html#basic-model",
    "title": "Estimating the impact of monetary policy shocks on different housing indicators: A Bayesian SVAR Approach",
    "section": "Basic Model",
    "text": "Basic Model\n\nPrior distribution\nGiven this parameterisation, we can write down the kernel of the prior given hyperparameters \\((\\underline B, \\underline \\Omega, \\underline S, \\underline \\nu)\\) as follows: \\[\n| \\det(B_0) |^{\\underline \\nu - N} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N b_n V_n \\underline S^{-1} V_n' b_n'  \\right\\}  \\exp \\left\\{  -\\frac{1}{2}  \\sum_{n=1}^N  \\left( B_{+n} - b_nV_n \\underline B \\right) \\underline \\Omega^{-1}  \\left( B_{+n} - b_nV_n \\underline B \\right)' \\right\\}  \n\\]\nThis prior distribution has two key advantages. Firstly, it leads to a full-conditional posterior distributions that allow efficient sampling. This allows us to estimate the structural parameters of the SVAR model.\nSecondly, it belongs to a class of reference prior distributions that are invariant to the pre-multiplication of the parameter matrices by a rotation matrix up to which the system is identified (see Rubio-Ramirez, Waggoner, and Zha (2010)). This allows us to conduct a proper Bayesian treatment of this model given the identification above.\n\n\n\n\nCalibration of the prior\n\n\\(\\underline \\nu = N\\) is a commonly chosen value as it implies that the generalised-normal part is equivalent to a \\(r_n\\)-variate normal with the mean equal to a vector of zeros and the covariance matrix equal to \\(\\underline S\\).\n\\(\\underline S = \\kappa_3 I_N\\) implies that the covariances across the rows of \\(B_0\\) is zero, and the variance of each row is homoskedastic (constant). \\(\\kappa_3\\) can be interpreted as a contemporaneous effects shrinkage and is set to 10.\n\\(\\underline B = [0_{N \\times 1} \\; \\kappa_4 I_N \\; 0_{N \\times (p-1)N}]\\) implies an AR1 process for the structural VAR at the prior mean. In this calibration, \\(\\kappa_4 = 1\\), the AR1 process is a random walk process.\n\\(\\underline \\Omega = \\begin{pmatrix} \\kappa_2 & 0\\\\ 0 & \\kappa_1 I_{Np} \\end{pmatrix}\\) is the prior covariance matrix. It is taken to be a diagonal matrix with the diagonal elements set as the Litterman prior. \\(\\kappa_2\\) represents the constant term shrinkage and is set to 10. \\(\\kappa_1\\) represents the autoregressive slope shrinkage and is set to 0.1.\n\nWe also calibrate the number of draws \\(S = 5000\\) for any sampling, while the \\(S.burnin = 100\\) represents the number of draws that are sampled first and then discarded.\nThe following R code creates a list of model parameters with the calibration as above.\n\n# set the priors\n\nparameters = list(\n  kappa1    = .1,       # autoregressive slope shrinkage\n  kappa2    = 10,       # constant term shrinkage\n  kappa3    = 10,       # contemporaneous effects shrinkage\n  kappa4    = 1,        # VAR prior persistence\n  S         = 5000,     # number of sample draws\n  S.burnin  = 100,      # number of initial draws that are burned-in\n  h         = 16        # forecast horizon\n)\n\nThe following R function takes as argument data and model parameters to compute parameters of the prior distribution and store it as a list.\n\n# A function that computes and stores all the prior distribution components given a parameter list input\nprior     = function(parameters, usedata){\n  priors  = list(\n  B       = cbind(rep(0,usedata$N), parameters$kappa4*diag(usedata$N), matrix(0, usedata$N, (usedata$p-1)*usedata$N)), # random walk prior\n  Omega   = diag(c(parameters$kappa2,parameters$kappa1*((1:usedata$p)^(-2))%x%rep(1,usedata$N))),\n  # Omega = diag(c(parameters$kappa2,parameters$kappa1*rep(1,usedata$N*usedata$p))),\n  S       = parameters$kappa3*diag(usedata$N),\n  nu      = usedata$N\n  )\n}\n\n\n\n\nLikelihood Function\nThe conditional normality of the error term allows us to write the kernel of the likelihood function and show that it can be expressed as a NGN distribution. \\[\n\\begin{align*}\n    &L(B_+, B_0|Y,X) \\propto | \\det(B_0^{-1}B_0^{-1'})|^{-\\frac{T}{2}} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N \\left(b_n V_n Y - B_{+n} X \\right) \\left( b_n V_n Y - B_{+n} X  \\right)' \\right\\}\\\\\n    & = | \\det(B_0)|^{T} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N \\left( b_n V_n Y Y' V_n' b_n' - 2 b_n V_n Y X' B_{+n}' + B_{+n} X X' B_{+n}' \\right) \\right\\}\\\\\n    & = | \\det(B_0)|^{T} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N \\left( b_n V_n Y Y' V_n' b_n' + B_{+n} X X' B_{+n}' - 2 b_n V_n Y X' (XX')^{-1} (XX') B_{+n}' \\right. \\right. \\\\\n    & \\left. \\left. + b_n V_n Y X' (XX')^{-1} (XX') (XX')^{-1} XY'V_n'b_n' - b_n V_n Y X' (XX')^{-1} (XX') (XX')^{-1} XY'V_n'b_n' \\right) \\right\\}\\\\\n    & = | \\det(B_0)|^{T} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N \\left( b_n V_n [ YY' - YX'(XX')^{-1}XY'] V_n' b_n'  + B_{+n} X X' B_{+n}' \\right. \\right. \\\\\n    & \\left. \\left. - 2 b_n V_n Y X' (XX')^{-1} (XX') B_{+n}' + b_n V_n Y X' (XX')^{-1} XY' V_n' b_n' \\right) \\right\\}\\\\\n    & \\text{}\\\\\n    & \\text{Let $\\hat A = YX' (XX')^{-1} $, then we can simplify}\\\\\n    & \\text{}\\\\\n    & =  | \\det(B_0)|^{T} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N \\left( b_n V_n [ YY' - \\hat A XY'] V_n' b_n'  + B_{+n} X X' B_{+n}' - 2 b_n V_n \\hat A (XX') B_{+n}' \\right. \\right.\\\\ & \\left. \\left. + b_n V_n \\hat A XY' V_n' b_n' \\right) \\right\\}\\\\\n    & =  | \\det(B_0)|^{T} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N \\left( b_n V_n [ YY' - \\hat A XY'] V_n' b_n'  + (B_{+n} - b_n V_n \\hat A) X X' (B_{+n} - b_n V_n \\hat A)' \\right) \\right\\}\\\\\n    & =  | \\det(B_0)|^{T} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N b_n V_n [ YY' - \\hat A XY'] V_n' b_n' \\right\\} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N (B_{+n} - b_n V_n \\hat A) X X' (B_{+n} - b_n V_n \\hat A)' \\right\\}\n\\end{align*}\n\\]\nComparing this with the general NGN kernel, we can write that\n\\[\nL(B_+, B_0|Y, X) = \\mathcal{NGN}(\\tilde B, \\tilde \\Omega, \\tilde S, \\tilde \\nu)\n\\] where\n\\[\n\\tilde B = \\hat A, \\quad \\tilde \\Omega = (XX')^{-1}, \\quad \\tilde S = ( YY' - \\hat A XY')^{-1}, \\quad \\tilde \\nu = T + N.\n\\] Hence, we have shown that the likelihood function follows a NGN distribution.\n\n\nPosterior Distribution\nThe prior and the likelihood can be used to obtain the posterior as follows:\n\\[\n\\begin{align*}\n    & p(B_+, B_0|Y, X)  \\propto L(B_+, B_0|Y, X) p(B_0, B_+)\\\\\n    & = | \\det(B_0)|^{T} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N \\left(b_n V_n Y - B_{+n} X \\right) \\left( b_n V_n Y - B_{+n} X  \\right)' \\right\\}\\\\\n    & \\times | \\det(B_0) |^{\\underline \\nu - N} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N b_n V_n \\underline S^{-1} V_n' b_n'  \\right\\}  \\exp \\left\\{  -\\frac{1}{2}  \\sum_{n=1}^N  \\left( B_{+n} - b_nV_n \\underline B \\right) \\underline \\Omega^{-1}  \\left( B_{+n} - b_nV_n \\underline B \\right)' \\right\\} \\\\\n    & =  | \\det(B_0) |^{T + \\underline \\nu - N} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N \\left(b_n V_n Y Y' V_n' b_n' - 2 b_n V_n Y X' B_{+n}' + B_{+n} X X' B_{+n}' \\right) \\right.\\\\\n    & \\left. + b_n V_n \\underline S^{-1} V_n' b_n' + B_{+n} \\underline \\Omega^{-1} B_{+n}' - 2 b_n V_n \\underline B \\underline \\Omega^{-1} B_{+n}' + b_n V_n \\underline B \\underline \\Omega^{-1} \\underline B' V_n' b_n' \\right\\}\\\\\n    & =  | \\det(B_0) |^{T + \\underline \\nu - N} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N b_n V_n [ YY' + \\underline S^{-1} + \\underline B \\underline \\Omega^{-1} \\underline B'] V_n' b_n' - 2 b_n V_n [ YX' + \\underline B \\underline \\Omega^{-1} ] B_{+n}' \\right.\\\\\n    & \\left. + B_{+n} [ XX' + \\underline \\Omega^{-1} ] B_{+n}' \\right\\}\\\\ & \\text{}\\\\\n    & \\text{ Let $ \\bar \\Omega = (XX' + \\underline \\Omega^{-1})^{-1} $, then we can write}\\\\ & \\text{}\\\\\n    & =  | \\det(B_0) |^{T + \\underline \\nu - N} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N b_n V_n [ YY' + \\underline S^{-1} + \\underline B \\underline \\Omega^{-1} \\underline B'] V_n' b_n' - 2 b_n V_n [ YX' + \\underline B \\underline \\Omega^{-1} ] \\bar \\Omega \\bar \\Omega^{-1} B_{+n}' \\right.\\\\\n    & \\left. + B_{+n} \\bar \\Omega^{-1} B_{+n}' \\right\\}\\\\\n    & \\text{}\\\\\n    & \\text{ Let $ \\bar B = (YX' + \\underline B \\underline \\Omega^{-1}) \\bar \\Omega $, then we can write}\\\\ & \\text{}\\\\\n    & =  | \\det(B_0) |^{T + \\underline \\nu - N} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N b_n V_n [ YY' + \\underline S^{-1} + \\underline B \\underline \\Omega^{-1} \\underline B' - \\bar B \\bar \\Omega^{-1} \\bar B' ] V_n' b_n'  \\right.\\\\\n    & \\left.  + b_n V_n  \\bar B \\bar \\Omega^{-1} \\bar B' V_n' b_n' - 2 b_n V_n \\bar B \\bar \\Omega^{-1} B_{+n}' + B_{+n} \\bar \\Omega^{-1} B_{+n}' \\right\\}\\\\\n    & \\text{}\\\\\n    & \\text{ Define $ \\bar S = (YY' + \\underline S^{-1} + \\underline B \\underline \\Omega^{-1} \\underline B' - \\bar B \\bar \\Omega^{-1} \\bar B')^{-1} $, then we can write}\\\\ & \\text{}\\\\\n    & =  | \\det(B_0) |^{T + \\underline \\nu - N} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N b_n V_n \\bar S^{-1} V_n' b_n'  + (B_{+n} - b_n V_n \\bar B) \\bar \\Omega^{-1} (B_{+n} - b_n V_n \\bar B)' \\right\\}\\\\\n    &  = | \\det(B_0) |^{T + \\underline \\nu - N} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N b_n V_n \\bar S^{-1} V_n' b_n'  \\right\\} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N (B_{+n} - b_n V_n \\bar B) \\bar \\Omega^{-1} (B_{+n} - b_n V_n \\bar B)' \\right\\}\n\\end{align*}\n\\] Thus, \\[ p(B_+, B_0|Y,X) \\sim \\mathcal{NGN}(\\bar B, \\bar \\Omega, \\bar S, \\bar \\nu) \\] where the first three parameters are defined above and \\(\\bar \\nu = T + \\underline \\nu\\).\nThe following R function uses the prior and information contained in data to compute and store (as a list) the set of posterior parameters for our model.\n\nposterior     = function(priors, usedata){\n  Omega.inv   = solve(priors$Omega)\n  Omega.post.inv = usedata$X%*%t(usedata$X) + Omega.inv\n  Omega.post  = solve( Omega.post.inv )\n  B.post      = (usedata$Y%*%t(usedata$X) + priors$B%*%Omega.inv) %*% Omega.post\n  S.post      = solve(usedata$Y%*%t(usedata$Y) + solve(priors$S) + priors$B%*%Omega.inv%*%t(priors$B) - B.post%*%Omega.post.inv%*%t(B.post) )\n  nu.post     = ncol(usedata$Y) + priors$nu\n\n  posteriors  = list(\n  B           = B.post,\n  Omega       = Omega.post,\n  S           = S.post,\n  nu          = nu.post\n)\n}\n\n\n\n(Gibbs) Sampling\nThe sampling algorithm follows the derivation by D. F. Waggoner and Zha (2003). The algorithm first samples \\(b_1, \\dots, b_n\\) independent of \\(B_+\\) in a serial, iterative fashion. Then, the sampled \\(b_1, \\dots, b_n\\) are normalised. Using the normalised \\(b_1, \\dots, b_n\\) and data, \\(B_{+n}\\) can be drawn independently.\n\nSample \\(b_1, \\dots, b_n\\) iteratively\nRecall that the marginal posterior distribution of \\(b_1, \\dots, b_n\\) is proportional to \\[\n| \\det(B_0) |^{\\underline \\nu - N} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N b_n V_n \\underline S^{-1} V_n' b_n'  \\right\\}\n\\] The Gibbs sampler draws from the full conditional posterior distribution of vector \\(b_n\\) given data as well as parameters from other rows of contemporaneous effects matrix, that is, \\(b_1, \\dots, b_{n-1}, b_{n+1}, \\dots, b_{N}\\). The full conditional posterior is denoted as \\[\np(b_n|Y, X, b_1, \\dots,  b_{n-1}, b_{n+1}, \\dots, b_{N})\n\\] To sample from the full conditional posterior \\(p(b_n^{(s)}|Y, X, b_1^{(s)}, \\dots, b_{n-1}^{(s)}, b_{n+1}^{(s-1)}, \\dots, b_{N}^{(s-1)})\\) at each iteration \\(s\\) and at each row \\(n\\), the following steps are undertaken:\nStep 1: define and compute the following values\n\n\\(U_n = chol \\left( \\bar \\nu (V_n \\bar S^{-1}V_n')^{-1} \\right)\\) – an \\(r_n \\times r_n\\) upper-triangular matrix;\n\\(w = B_{0[-n.\\cdot]\\perp}^{(s)}\\) – a \\(1 \\times N\\) matrix;\n\\(w_1 = wV_n'U-n' \\cdot \\left( w V_n'U_n'U_n V_n w' \\right)^{-\\frac{1}{2}}\\) – a \\(1 \\times r_n\\) vector;\n\\(W_n = [w_1' \\quad w_{1 \\perp}']'\\) – a \\(r_n \\times r_n\\) matrix.\n\nStep 2: draw the elements of a \\(1 \\times r_n\\) vector \\(\\alpha_n\\) defined as follows: - draw the first element by drawing $u ({+1}, ^{-1}I{+1}) and setting \\[\n\\alpha_{n[\\cdot.1]} =\n\\begin{cases}\n\\sqrt{u'u} & \\text{ with probability } 0.5\\\\\n-\\sqrt{u'u} & \\text{ with probability } 0.5\n\\end{cases}\n\\] - draw the remaining \\(r_n-1\\) element of \\(\\alpha_n\\) from \\(\\mathcal{N}(\\textbf{0}_{r_n-1}, \\bar \\nu^{-1} I_{r_n-1})\\).\nStep3: compute the draw from the full-conditional posterior distribution of \\(b_n\\) by \\[\nb_n^{(s)} = \\alpha_n W_n U_n\n\\]\nNote that \\(X\\perp\\) refers to the orthogonal-complement matrix of \\(X\\). And, \\(B_{0[-n.\\cdot]}\\) refers to the matrix \\(B_{0}\\) without its n\\(^{th}\\) row.\n\n\nNormalise \\(b_1, \\dots, b_n\\)\nD. Waggoner and Zha (2003) provide a normalising rule that preserves the shape of the likelihood function. These normalised draws from the normal-generalised-normal posterior distribution are free of the local identification problem and hence the estimates post-normalisation are meaningful and appropriate for statistical inference.\nStep 1: normalise with respect to one mode \\(\\hat B_0\\) of the posterior distribution. Let \\(\\hat B_0\\) be defined as \\[\n  \\hat B_0 = chol((\\bar\\nu-N)*\\bar S)'\n\\]\nStep 2: define scaling matrices \\(D_i\\) for \\(i=1,\\cdot,2^N\\).\nThese \\(N \\times N\\) matrices \\(D_i\\) are diagonal matrices with diagonal elements equal to -1 or 1. Thus, \\(2^N\\) set of \\(D_i\\) matrices cover all possible combinations of -1 and 1 on the diagonal.\nStep 3: compute the distance between \\(D_i B_0^{(s)}\\) and \\(\\hat B_0\\).\n\\[\nd \\left( \\left[ \\left( D_i B_0^{(s)} \\right)^{-1'} - \\hat B_0^{-1'}   \\right] | (\\hat B_0' \\hat B_0)^{-1}   \\right) = \\sum_{n=1}^N \\left[ \\left( D_i B_0^{(s)} \\right)^{-1'} - \\hat B_0^{-1'}   \\right]_{[n. \\cdot]} (\\hat B_0' \\hat B_0)^{-1}  \\left[ \\left( D_i B_0^{(s)} \\right)^{-1'} - \\hat B_0^{-1'}   \\right]_{[n. \\cdot]}'\n\\]\nThe choice of \\(D_i\\) that minimises this distance is used to create the normalised draw \\(D_{i*(s)}B_0^{(s)}\\) after applying it to all of the \\(S\\) draws.\n\n\nSample \\(B_{+n}\\) independently\nFor each draw of \\(b_n^{(s)}\\), a corresponding draw of \\(B_{+n}^{(s)}\\) is directly sampled from the normal distribution below: \\[\nB_{+n}^{(s)} \\sim \\mathcal{N}(b_n^{(s)}V_n \\bar B_n, \\bar Omega)\n\\]\nThe posteriorSimuations function below takes model parameters, the posterior parameters, the exclusion restriction and the number of variables as given to sample draws using the Gibbs sampler. The functions rgn, normalize.Gibbs.output.parallel and rnorm.ngn are obtained from Tomasz Woźniak’s lecture notes. These functions are provided in the Appendix.\n\nposteriorSimulations  = function(parameters, posteriors, B0Vlist, N){\n  t0                  = proc.time()\n  B0.posterior        = rgn(n=parameters$S.burnin, S.inv=solve(posteriors$S), nu=posteriors$nu, V=B0Vlist$V, B0.initial=B0Vlist$B0.initial)\n  t1                  = proc.time()\n  (t1-t0)/60\n  \n  # sampling B0 from the posterior distribution using Gibbs\n  t0                  = proc.time()\n  B0.posterior        = rgn(n=parameters$S, S.inv=solve(posteriors$S), nu=posteriors$nu, V=B0Vlist$V, B0.initial=B0.posterior[,,parameters$S.burnin])\n  t1                  = proc.time()\n  (t1-t0)/60\n  \n  # normalisation\n  B0.hat              = t(chol((posteriors$nu-N)*posteriors$S))                   # normalisation using this B0.hat should work\n  BM.B0.posterior     = normalize.Gibbs.output.parallel(B0.posterior,B0.hat=B0.hat)\n  t2                  = proc.time()\n  (t2-t1)/60\n  \n  # sample B+ from the normal conditional posterior\n  t2                  = proc.time()\n  BM.Bp.posterior     = rnorm.ngn(BM.B0.posterior, B=posteriors$B,Omega=posteriors$Omega)\n  t3                  = proc.time()\n  (t3-t2)/60\n  \n  list(B0.posterior   = BM.B0.posterior, Bp.posterior = BM.Bp.posterior)\n}\n\nThe posteriorMeans function below computes the sample average of the \\(B_0\\) and \\(B_+\\) matrices obtained from each sampling step.\n\nposteriorMeans = function(Bposteriors){\n  Bposteriors.means = list(\n  B0 = rowMeans(Bposteriors$B0.posterior, dims = 2),\n  Bp = rowMeans(Bposteriors$Bp.posterior, dims = 2)\n  )\n}\n\nThe pmatrix function below prints the matrices in R in \\(\\Latex\\) form.\n\npmatrix <- function(x) {\n  cat(c(\"$$\\\\begin{equation*}\\n\",\n    \"\\\\left(\",\n    knitr::kable(x, format = \"latex\", \n                 tabular = \"array\",\n                 vline = \"\",\n                 align = \"c\",\n                 linesep = \"\",\n                 toprule = NULL,\n                 bottomrule = NULL),\n    \"\\n\\\\right)\\\\, .\\n\",\n    \"\\\\end{equation*}$$\\n\"))\n}\n\n\n\n\nSimulation Run\nThe purpose of this simulation run is to verify whether my model and corresponding code can replicate the true parameters of a data-generating process. To do this, I create artificial data containing 1000 observations simulated from a bi-variate Gaussian random walk process with the covariance matrix equal to the identity matrix of order 2. Then, I estimate a model with a constant term and 1 lag with the artificial data. And I show that the posterior mean of the autoregressive and the covariance matrices are close to an identity matrix and that the posterior mean of the constant term is close to a vector of zeros.\nThis process is laid out in steps below:\nStep 1: The following code generates artificial data containing 1000 observations from a bi-variate Gaussian random walk process with the covariance matrix equal to the identity matrix of order 2.\n\nset.seed(12345)\n\nsim.p = 1\nsim.T = 1000\nsim.N = 2\nsim.K = 1 + sim.N*sim.p\n\nsim.Y           = arima.sim(list(order = c(0,1,0)), n = sim.T + sim.p-1, mean = 0, sd =1)\nfor (i in 2:sim.N){\n  sim.Y         = rbind(sim.Y, arima.sim(list(order = c(0,1,0)), n = sim.T + sim.p-1, mean = 0, sd = 1))\n}\n\nsim.X           = matrix(1,1,sim.T)\nfor (i in 1:sim.p){\n  sim.X         = rbind(sim.X, sim.Y[,(sim.p+1-i):(ncol(sim.Y)-i)])\n}\nsim.Y           = sim.Y[,-sim.p]\nartificialdata  = list(p = sim.p, N = sim.N, K = sim.K, Y = sim.Y, X = sim.X)\n\nStep 2: We obtain a list of simulation priors and posteriors using the prior and posterior functions.\n\nsim.priors      = prior(parameters, artificialdata)\nsim.posteriors  = posterior(sim.priors, artificialdata)\n\nStep 3: We create a list of \\(V_n\\) and \\(b_n\\) corresponding to a lower triangular exclusion restriction on \\(B_0\\) using the ltexclusion function.\n\nsim.B0Vlist   = ltexclusion(artificialdata)\n\nStep 4: We sample the \\(B_0\\) and \\(B_p\\) posteriors with the Gibbs sampler using the posteriorSimulations function, and save the results for future use.\n\nsim.Bposteriors       = posteriorSimulations(parameters, sim.posteriors, sim.B0Vlist, artificialdata$N)\nsave(sim.Bposteriors, sim.priors, sim.posteriors, file = \"sim-posteriors.RData\")\n\n\n\n\nStep 5: We compute the sample averages of our posterior \\(B_0\\) and \\(B_+\\). We use the pmatrix function to display the results as a matrix.\nsim.Bposteriors.means = posteriorMeans(sim.Bposteriors)\nsim_B0 = pmatrix(sim.Bposteriors.means$B0)\n\\[\\begin{equation*}\n\\left(\n\\begin{array}{cc}\n1.007094 & 0.000000\\\\\n-0.037662 & 1.000047\\\\\n\\end{array}\n\\right)\\, .\n\\end{equation*}\\]\nWe can see that the computed \\(B_0\\) covariance matrix is numerically identical to an identity matrix.\nsim_Bp = pmatrix(sim.Bposteriors.means$Bp)\n\\[\\begin{equation*}\n\\left(\n\\begin{array}{ccc}\n0.2276255 & 0.9989406 & -0.0071993\\\\\n-0.1389968 & -0.0461782 & 0.9791025\\\\\n\\end{array}\n\\right)\\, .\n\\end{equation*}\\]\nThe first column of \\(B_+\\) represents the posterior mean of the constant term. The values are small and close to zero. The rest of the \\(B_+\\) matrix represents the autoregressive matrix. Its posterior mean is numerically equal to an identity matrix.\n\n\nData Results\nStep 1: I set the desired number of lags in the model and use the data to create matrices \\(X\\) and \\(Y\\). I store my data as a list named mydata.\n\n# Y is N by T; X is K by T\n\np = 4                                   # set a number of lags included\nN = ncol(df)\nK = 1 + N*p\n\nY = t(df[(p+1):nrow(df),])\nX = matrix(1,1,ncol(Y))\n\nfor (i in 1:p){\n  X    = rbind(X,t(df[((p+1):nrow(df))-i,]))\n}\n\nmydata = list(p=p,N=N,K=K,Y=Y,X=X)\n\nStep 2: We obtain a list of data priors and posteriors using the prior and posterior functions.\n\npriors     = prior(parameters, mydata)\nposteriors = posterior(priors, mydata)\n\nStep 3: We create a list of \\(V_n\\) and \\(b_n\\) corresponding to a lower triangular exclusion restriction on \\(B_0\\) using the ltexclusion function.\n\nB0Vlist       = ltexclusion(mydata)\n\nStep 4: We sample the \\(B_0\\) and \\(B_p\\) posteriors with the Gibbs sampler using the posteriorSimulations function, and save the results for future use.\n\nBposteriors           = posteriorSimulations(parameters, posteriors, B0Vlist, mydata$N)\nsave(Bposteriors, priors, posteriors, file = \"data-posteriors.RData\")\n\n\n\n\nStep 5: We compute the sample averages of our posterior \\(B_0\\) and \\(B_+\\). We use the pmatrix function to display the results as a matrix.\nBposterior.means = posteriorMeans(Bposteriors)\ndata_B0 = pmatrix(Bposterior.means$B0)\n\\[\\begin{equation*}\n\\left(\n\\begin{array}{ccccccccc}\n2.7194046 & 0.0000000 & 0.0000000 & 0.0000000 & 0.0000000 & 0.0000000 & 0.0000000 & 0.000000 & 0.0000\\\\\n-0.0083644 & 26.4727032 & 0.0000000 & 0.0000000 & 0.0000000 & 0.0000000 & 0.0000000 & 0.000000 & 0.0000\\\\\n0.0008651 & -3.3422037 & 25.2345905 & 0.0000000 & 0.0000000 & 0.0000000 & 0.0000000 & 0.000000 & 0.0000\\\\\n-0.1828049 & -0.7020495 & 1.3466381 & 1.7545424 & 0.0000000 & 0.0000000 & 0.0000000 & 0.000000 & 0.0000\\\\\n-0.0860228 & 0.8414400 & -0.5550294 & -0.3932550 & 2.7677422 & 0.0000000 & 0.0000000 & 0.000000 & 0.0000\\\\\n-0.0356738 & -0.0524865 & 0.4328343 & -0.0207150 & 0.7724282 & 23.5464079 & 0.0000000 & 0.000000 & 0.0000\\\\\n0.0564714 & -0.0591610 & -1.1540135 & -0.1889706 & -11.7074445 & 0.5748700 & 12.5986869 & 0.000000 & 0.0000\\\\\n-0.0350259 & 1.0455056 & -0.4176768 & 0.2197704 & -0.7367318 & -0.7794951 & 0.5702032 & 18.973736 & 0.0000\\\\\n-0.1514784 & -0.5314789 & -0.8653124 & -0.2656810 & -0.1211242 & -1.7226130 & 0.0432794 & -1.088046 & 25.4726\\\\\n\\end{array}\n\\right)\\, .\n\\end{equation*}\\]\ndata_Bp = pmatrix(Bposterior.means$Bp)\n\\[\\begin{equation*}\n\\left(\n\\begin{array}{ccccccccccccccccccccccccccccccccccccc}\n4.0648312 & 2.8851434 & -0.0720763 & 0.0543550 & -0.1063726 & 0.1452214 & -0.2101715 & -0.3061815 & 0.1386728 & -0.0491164 & -0.0825012 & -0.0205374 & 0.0198261 & 0.0963603 & 0.0343161 & -0.0529068 & -0.0654098 & 0.0386580 & -0.0022661 & -0.1084682 & -0.0085254 & 0.0067943 & -0.0118303 & 0.0085827 & -0.0164046 & -0.0325035 & 0.0195501 & -0.0021792 & -0.1311530 & -0.0044261 & 0.0038886 & -0.0031745 & 0.0019857 & -0.0107474 & -0.0170848 & 0.0107877 & 0.0008287\\\\\n-0.2029448 & 0.0354287 & 26.4521943 & -0.0219235 & 0.1490847 & 0.0251019 & 0.0267879 & 0.0546776 & 0.0632533 & 0.0058823 & 0.0088559 & -0.0042048 & -0.0085999 & -0.1017568 & 0.0058958 & 0.0119987 & 0.0077623 & 0.0098798 & 0.0011410 & -0.0204612 & -0.0007752 & -0.0035664 & -0.0191747 & -0.0096680 & 0.0051670 & -0.0031704 & 0.0043929 & -0.0018771 & -0.0091002 & -0.0005856 & 0.0006959 & -0.0103657 & -0.0142938 & 0.0018546 & -0.0118245 & 0.0000619 & 0.0009664\\\\\n0.1940443 & 0.0467739 & -3.3574638 & 25.2143267 & 0.0714187 & 0.0251060 & -0.0184962 & -0.0075943 & 0.0571311 & -0.0033328 & 0.0101925 & -0.0018344 & -0.0042200 & -0.1498857 & 0.0103576 & -0.0018948 & 0.0028173 & 0.0031057 & 0.0030597 & -0.0430409 & 0.0032038 & 0.0035788 & 0.0149999 & -0.0016695 & -0.0021676 & 0.0047587 & -0.0035240 & 0.0001813 & -0.0177420 & 0.0012195 & 0.0023508 & 0.0207860 & 0.0052478 & -0.0001616 & 0.0072795 & 0.0007489 & 0.0024354\\\\\n-0.2994919 & -0.1046635 & -0.7076678 & 1.3192450 & 1.2545097 & 0.2219172 & 0.0819993 & 0.2342432 & 0.1860287 & 0.0134389 & -0.0665931 & -0.0068933 & -0.0133227 & -0.1690935 & -0.0475242 & 0.0273171 & -0.0270672 & 0.0256128 & -0.0012948 & 0.0172428 & -0.0052248 & -0.0089509 & -0.1366887 & -0.0386805 & 0.0055421 & -0.0350628 & 0.0032266 & -0.0056818 & -0.0058723 & 0.0006462 & -0.0025096 & -0.0743186 & -0.0082846 & 0.0046185 & -0.0024328 & -0.0073177 & -0.0046012\\\\\n2.5585255 & -0.0066754 & 0.8288711 & -0.5020729 & -0.4568230 & 3.0138531 & -0.1564680 & -0.1677028 & 0.1617219 & 0.0217405 & 0.0475507 & -0.0020607 & 0.0101789 & -0.0737170 & -0.0415982 & -0.0306631 & -0.1287063 & 0.0263266 & -0.0050378 & 0.0155091 & -0.0015822 & 0.0057429 & -0.0500014 & -0.0438827 & -0.0135053 & -0.0733431 & 0.0057693 & -0.0068353 & -0.0591949 & -0.0020136 & -0.0005626 & 0.0232184 & 0.0027831 & -0.0094360 & -0.0070513 & 0.0012375 & -0.0054753\\\\\n0.7397746 & 0.0137152 & -0.0229393 & 0.4558805 & -0.0193145 & 0.7315018 & 23.5552900 & -0.0139548 & -0.0115209 & -0.0265769 & 0.0120738 & 0.0050422 & 0.0078787 & -0.0125916 & -0.0357843 & 0.0066207 & -0.0293907 & -0.0039903 & -0.0101668 & 0.0358080 & 0.0064970 & -0.0007591 & 0.0043771 & 0.0026772 & 0.0002161 & 0.0047751 & 0.0003684 & -0.0044988 & 0.0193735 & 0.0015446 & 0.0000531 & -0.0130743 & -0.0050762 & 0.0030893 & -0.0074677 & 0.0011153 & -0.0027520\\\\\n-0.8698116 & -0.0773580 & -0.0332854 & -1.1619398 & -0.2286229 & -11.6541655 & 0.6447739 & 12.6408380 & -0.0523751 & 0.0345840 & 0.0138937 & 0.0141900 & 0.0009504 & -0.0940291 & -0.0251588 & 0.0188155 & -0.0237115 & -0.0096623 & 0.0090631 & 0.0862063 & 0.0056785 & 0.0018945 & 0.0153264 & 0.0233680 & 0.0061449 & 0.0209966 & -0.0069842 & 0.0019724 & 0.0578864 & 0.0006781 & -0.0005237 & -0.0000858 & 0.0155333 & 0.0049881 & 0.0147274 & -0.0012528 & 0.0030459\\\\\n1.6144229 & -0.0908928 & 1.0580411 & -0.4165733 & -0.3273910 & -0.8469357 & -0.7737012 & 0.3943126 & 18.8998265 & -0.0680833 & 0.0039033 & 0.0073150 & 0.0052320 & 0.0100443 & 0.0334497 & 0.0002363 & 0.0119692 & -0.0546084 & -0.0228015 & 0.0312552 & 0.0059630 & 0.0084129 & 0.0467699 & 0.0723329 & -0.0096091 & 0.0632058 & -0.0246182 & -0.0107407 & 0.0388868 & 0.0037111 & 0.0053680 & 0.0654332 & 0.0740202 & -0.0081970 & 0.0648373 & -0.0110446 & -0.0038691\\\\\n0.6191889 & -0.1450130 & -0.5229711 & -0.8750196 & -0.2189709 & -0.1517908 & -1.6845184 & -0.0784696 & -0.9996365 & 25.4435492 & 0.0072336 & -0.0021376 & -0.0053391 & -0.0242130 & -0.0060620 & 0.0054626 & -0.0154996 & 0.0136988 & -0.0144934 & 0.0000250 & -0.0001331 & -0.0019112 & 0.0093214 & 0.0214501 & 0.0022844 & 0.0144202 & 0.0005166 & -0.0132830 & 0.0128877 & -0.0033314 & -0.0028081 & 0.0272992 & 0.0229197 & 0.0021625 & 0.0178043 & -0.0034796 & -0.0063407\\\\\n\\end{array}\n\\right)\\, .\n\\end{equation*}\\]"
  },
  {
    "objectID": "index.html#extended-model",
    "href": "index.html#extended-model",
    "title": "Estimating the impact of monetary policy shocks on different housing indicators: A Bayesian SVAR Approach",
    "section": "Extended Model",
    "text": "Extended Model\nIn the extended model, I will estimate the hyperparameters rather than setting them exogenously. Such estimation procedure often improves the fit of the model especially because the results can be sensitive to the parameterisation of the hyperparameters. In particular, I estimate \\((\\kappa_0, \\kappa_+)\\) such that \\(\\underline S = \\kappa_0 I_N\\) and \\(\\underline \\Omega = \\kappa_+ I_K\\).\n\nPrior Distribution\nI postulate the following prior distributions for the hyperparameters: \\[\n\\begin{align*}\n    \\kappa_0 | \\underline s_{\\kappa_0}, \\underline \\nu_{\\kappa_0} \\sim \\mathcal{IG}2(\\underline s_{\\kappa_0}, \\underline \\nu_{\\kappa_0})  && \\kappa_+ | \\underline s_{\\kappa_+}, \\underline \\nu_{\\kappa_+} \\sim \\mathcal{G}(2 \\underline s_{\\kappa_+}, \\frac{1}{2} \\underline \\nu_{\\kappa_+})\n  \\end{align*}\n\\]\nThe ext.priors function creates a list containing all the elements of the prior.\n\next.priors = function(parameters,usedata){\n  ext.prior = list(\n  kappa0.s = .1,\n  kappa0.nu = 1,\n  kappap.s = .1,\n  kappap.nu = 1,\n  B       = cbind(rep(0,usedata$N), diag(usedata$N), matrix(0, usedata$N, (usedata$p-1)*usedata$N)), # random walk prior\n  Omega   = parameters$kappa2 * diag(usedata$K),\n  S       = parameters$kappa3*diag(usedata$N),\n  nu      = usedata$N\n)\n}\n\nMoreover, we have \\[\n  \\begin{align*}\n    p(B_{+n}|b_n, \\kappa_+) = \\mathcal{N}_k (b_n V_n B, \\kappa_+ \\Omega) && p(b_n| \\kappa_0) = \\mathcal{N}_{r_n}(0, \\kappa_0 S)\n  \\end{align*}\n\\]\n\n\nPosterior Distribution\nThus, posteriors can be written as \\[\n  \\begin{align*}\n    & p(\\kappa_0 | Y, X, B_0) \\propto p(B_0|\\kappa_0) p(\\kappa_0 | \\underline s_{\\kappa_0}, \\underline \\nu_{\\kappa_0} )\\\\\n    & \\propto \\prod_{n=1}^{N}\\left(\\kappa_0^{-\\frac{1}{2}}\\right)^{r_n} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N b_n V_n (\\kappa_0 I_N)^{-1} V_n' b_n'  \\right\\} \\cdot \\kappa_0^{-\\frac{\\underline \\nu_{\\kappa_0}+2}{2}} \\exp \\left\\{  -\\frac{\\underline s_{\\kappa_0}}{2 \\kappa_0} \\right\\}\\\\\n    & \\propto \\kappa_0^{ -\\frac{\\underline \\nu_{\\kappa_0} + 2 + \\sum_{n=1}^{N}r_n}{2}} \\cdot \\exp \\left\\{  -\\frac{1}{2 \\kappa_0} \\sum_{n=1}^N b_n V_n V_n' b_n' + \\underline s_{\\kappa_0} \\right\\}\\\\\n  \\end{align*}\n\\] This gives \\[\\bar s_{\\kappa_0} =  \\sum_{n=1}^N b_n V_n V_n' b_n' + \\underline s_{\\kappa_0}.\\] \\[\\bar \\nu_{\\kappa_0} = \\underline \\nu_{\\kappa_0} +\\sum_{n=1}^{N}r_n\\] Also, \\[\n  \\begin{align*}\n    & p(\\kappa_+ | Y, X, B_+, B_0, \\kappa_0) \\propto p(B_+|\\kappa_+, B_0, \\kappa_0) p(\\kappa_+ | \\underline s_{\\kappa_+}, \\underline \\nu_{\\kappa_+} )\\\\\n    & \\propto \\kappa_+^{-\\frac{KN}{2}} \\exp \\left\\{  -\\frac{1}{2} \\sum_{n=1}^N (B_{+n} - b_n V_n \\underline B) (\\kappa_+ I_{K})^{-1} (B_{+n} - b_n V_n \\underline B)'  \\right\\} \\cdot \\kappa_+^{-\\frac{\\underline \\nu_{\\kappa_+} + 2}{2}} \\exp \\left\\{  -\\frac{ \\kappa_+}{2 \\underline s_{\\kappa_+}} \\right\\}\\\\\n    & = \\kappa_+^{-\\frac{\\underline \\nu_{\\kappa_+} + KN}{2} - 1} \\cdot \\exp \\left\\{  -\\frac{1}{2} \\left( (B_{+n} - b_n V_n \\underline B)  (B_{+n} - b_n V_n \\underline B)' \\cdot \\frac{1}{\\kappa_+} + \\frac{1}{\\underline s_{\\kappa_+}} \\kappa_+  \\right) \\right\\}\\\\\n  \\end{align*}\n\\] This gives \\[\\lambda = -\\frac{\\underline \\nu_{\\kappa_+} + KN}{2}\\] \\[\\chi =  \\sum_{n=1}^N  (B_{+n} - b_n V_n \\underline B)  (B_{+n} - b_n V_n \\underline B)'\\] \\[\\Psi = \\frac{1}{\\underline s_{\\kappa_+}}\\]\nThe init.struct function creates and initialise a list that contains matrices to store draws of \\(\\{\\kappa_0^{(s)}, \\kappa_+^{(s)}, B_0^{(s)}, B_+^{(s)}\\}_{s =1}^{S}\\).\n\n  init.struct = function(usedata,S){\n    kappa0 = array(NA,c(1,S))\n    kappa0[1] = 10\n    kappap = rep(NA, S)\n    kappap[1] = 10\n    B0.posterior = array(NA, c(usedata$N,usedata$N,S))\n    Bp.posterior = array(NA, c(usedata$N,usedata$K,S))\n    list(kappa0 = kappa0, kappap = kappap, B0.posterior = B0.posterior, Bp.posterior = Bp.posterior)\n  }\n\n\n\n(Gibbs) Sampling\nThe sampling function ext.sampling is given below. The sampling procedure is as\n\next.sampling = function(parameters, struct, priors, usedata){\n  set.seed(12345)\n  B0Vlist.initial = ltexclusion(usedata)\n  B0.initial = B0Vlist.initial$B0\n  for (i in 1:(parameters$S + parameters$S.burnin)){\n    \n    # Computing posterior parameters for each draw\n    Omega.inv      = solve(struct$kappap[i] * priors$Omega)\n    Omega.post.inv = usedata$X%*%t(usedata$X) + Omega.inv\n    Omega.post     = solve(Omega.post.inv)\n    B.post         = (usedata$Y%*%t(usedata$X) + priors$B%*%Omega.inv) %*% Omega.post\n    S.post         = solve(usedata$Y%*%t(usedata$Y) + solve(struct$kappa0[i] * priors$S) + priors$B%*%Omega.inv%*%t(priors$B) - B.post%*%Omega.post.inv%*%t(B.post) )\n    nu.post        = ncol(usedata$Y) + priors$nu\n\n\n  if (i > 1){\n    B0.initial = struct$B0.post[,,i-1]\n  }\n  \n  B0.i = rgn(n=1, S.inv = solve(S.post), nu = nu.post, V = B0Vlist.initial$V, B0.initial = B0.initial)\n  B0.hat = t(chol((nu.post - usedata$N)*S.post))\n  B0.norm.i = normalize.Gibbs.output.parallel(B0.i, B0.hat)\n  Bp.i = rnorm.ngn(B0.norm.i, B.post, Omega.post)\n  \n  struct$B0.posterior[,,i] = B0.norm.i\n  struct$Bp.posterior[,,i] = Bp.i\n  \n  # Now, update kappa\n  kappa0.nu.post = priors$kappa0.nu + (usedata$N/2)\n  kappap.lambda.post = -(priors$kappap.nu + usedata$K * usedata$N)/2\n  kappap.psi.post = 1/priors$kappap.s\n  kappa0.s.post = priors$kappa0.s\n  # kappap.chi.post = 0\n  for (n in 1:usedata$N){\n    # kappa0.s.post = kappa0.s.post + B0.norm.i[n,,1] %*% t( B0.norm.i[n,,1])\n    kappa0.s.post = kappa0.s.post + sum(B0.norm.i[n,,1]^2)\n    kappap.chi.post = (Bp.i[n,,1] - B0.norm.i[n,,1] %*% B.post) %*% t(Bp.i[n,,1] - B0.norm.i[n,,1] %*% B.post)\n  }\n  \n  if (i != (parameters$S + parameters$S.burnin)){\n    struct$kappa0[i+1] = kappa0.s.post / rchisq(n=1,df = kappa0.nu.post)\n    struct$kappap[i+1] = GIGrvg::rgig(n=1, kappap.lambda.post, kappap.chi.post, kappap.psi.post)\n  }\n  }\n  struct$kappa0 = struct$kappa0[(parameters$S.burnin+1):(parameters$S.burnin + parameters$S)]\n  struct$kappap = struct$kappap[(parameters$S.burnin+1):(parameters$S.burnin + parameters$S)]\n  struct$B0.posterior = struct$B0.posterior[,,(parameters$S.burnin+1):(parameters$S.burnin + parameters$S)]\n  struct$Bp.posterior = struct$Bp.posterior[,,(parameters$S.burnin+1):(parameters$S.burnin + parameters$S)]\n  return(struct)\n  \n}\n\n\nstructPosteriorMeans = function(struct){\n  struct.means = list(\n  kappa0 = mean(struct$kappa0),\n  kappap = mean(struct$kappap),\n  B0 = rowMeans(struct$B0.posterior, dims = 2),\n  Bp = rowMeans(struct$Bp.posterior, dims = 2)\n  )\n}\n\n\n\nSimluation Run\nStep 1: The following code computes the prior, initialises matrices to store results, and samples the draws for the simulated data.\n\next.sim.prior = ext.priors(parameters, artificialdata)\next.sim.struct = init.struct(artificialdata, parameters$S + parameters$S.burnin)\next.sim.struct <- ext.sampling(parameters, ext.sim.struct, ext.sim.prior, artificialdata)\n\n\n\n\n\n\n\nStep 2: The following code computes and stores the posterior means of \\(\\{\\kappa_0^{(s)}, \\kappa_+^{(s)}, B_0^{(s)}, B_+^{(s)}\\}_{s =1}^{S}\\).\n\next.sim.struct.means = structPosteriorMeans(ext.sim.struct)\n\nStep 3: The code snippets below display the sample posterior means of the estimated objects using simulated data.\nThe following code computes the posterior mean of \\(B_0\\).\next.sim.data_B0 = pmatrix(ext.sim.struct.means$B0)\n\\[\\begin{equation*}\n\\left(\n\\begin{array}{cc}\n1.0031219 & 0.0000000\\\\\n-0.0354609 & 0.9974601\\\\\n\\end{array}\n\\right)\\, .\n\\end{equation*}\\]\nThe following code computes the posterior mean of \\(B_+\\).\next.sim.data_Bp = pmatrix(ext.sim.struct.means$Bp)\n\\[\\begin{equation*}\n\\left(\n\\begin{array}{ccc}\n0.0194938 & 0.9996890 & -0.0063959\\\\\n-0.0120185 & -0.0422528 & 0.9847704\\\\\n\\end{array}\n\\right)\\, .\n\\end{equation*}\\]\nThe following code computes the posterior mean of \\(\\kappa_0\\).\next.sim.data_kappa0 = pmatrix(ext.sim.struct.means$kappa0)\n\\[\\begin{equation*}\n\\left(\n\\begin{array}{c}\nx\\\\\n\\hline\n12.17841\\\\\n\\end{array}\n\\right)\\, .\n\\end{equation*}\\]\nThe following code computes the posterior mean of \\(\\kappa_+\\).\next.sim.data_kappap = pmatrix(ext.sim.struct.means$kappap)\n\\[\\begin{equation*}\n\\left(\n\\begin{array}{c}\nx\\\\\n\\hline\n0.0001206\\\\\n\\end{array}\n\\right)\\, .\n\\end{equation*}\\]\n\n\nData Results\nStep 1: The following code computes the prior, initialises matrices to store results, and samples the draws for the actual data.\n\n\n\n\n\n\n\n\n\nStep 2: The following code computes and stores the posterior means of \\(\\{\\kappa_0^{(s)}, \\kappa_+^{(s)}, B_0^{(s)}, B_+^{(s)}\\}_{s =1}^{S}\\).\n\next.struct.means = structPosteriorMeans(ext.struct)\n\nStep 3: The code snippets below display the sample posterior means of the estimated objects using actual data.\next.data_B0 = pmatrix(ext.struct.means$B0)\n\\[\\begin{equation*}\n\\left(\n\\begin{array}{ccccccccc}\n2.5321875 & 0.000000 & 0.000000 & 0.0000000 & 0.000000 & 0.0000000 & 0.0000000 & 0.000000 & 0.00000\\\\\n-0.0443897 & 68.550076 & 0.000000 & 0.0000000 & 0.000000 & 0.0000000 & 0.0000000 & 0.000000 & 0.00000\\\\\n-0.0327047 & -83.955580 & 81.027130 & 0.0000000 & 0.000000 & 0.0000000 & 0.0000000 & 0.000000 & 0.00000\\\\\n-0.0996414 & -17.670289 & 12.691657 & 1.5888635 & 0.000000 & 0.0000000 & 0.0000000 & 0.000000 & 0.00000\\\\\n-0.0603246 & 19.398118 & -15.375680 & -0.5509407 & 2.606298 & 0.0000000 & 0.0000000 & 0.000000 & 0.00000\\\\\n-0.1799992 & -10.005352 & 9.741918 & -0.0286774 & 1.249410 & 40.9518799 & 0.0000000 & 0.000000 & 0.00000\\\\\n0.5187241 & 12.322818 & -13.827839 & -0.2658787 & -15.520436 & 0.7319055 & 16.4520737 & 0.000000 & 0.00000\\\\\n-0.2154222 & 64.070527 & -37.641350 & -0.0793310 & -1.306610 & -6.9133303 & 1.3141142 & 24.055840 & 0.00000\\\\\n-0.3167300 & 3.503228 & -14.229969 & -0.6151953 & -1.154688 & -15.4474005 & 0.6823034 & -4.758896 & 61.02273\\\\\n\\end{array}\n\\right)\\, .\n\\end{equation*}\\]\next.data_Bp = pmatrix(ext.struct.means$Bp)\n\\[\\begin{equation*}\n\\left(\n\\begin{array}{ccccccccccccccccccccccccccccccccccccc}\n0.0029251 & 2.5481268 & 0.002333 & 0.0080512 & -0.0117987 & 0.0032651 & -0.0016894 & -0.0122848 & 0.0073596 & 0.0017226 & -0.0076761 & 0.0017804 & 0.0062764 & 0.0078701 & 0.0021727 & -0.0035716 & -0.0103360 & 0.0074675 & 0.0026433 & -0.0312672 & 0.0022980 & 0.0067363 & -0.0021688 & 0.0022338 & -0.0006890 & -0.0078465 & 0.0071517 & 0.0024689 & -0.0647060 & 0.0009101 & 0.0074270 & 0.0027232 & 0.0031327 & -0.0005021 & -0.0050311 & 0.0072036 & 0.0019251\\\\\n0.0003466 & -0.0204828 & 68.552505 & 0.0026284 & 0.0413168 & 0.0322458 & 0.0057413 & 0.0310140 & 0.0073795 & 0.0044696 & 0.0219853 & 0.0024146 & 0.0037416 & -0.0296910 & 0.0228440 & 0.0062357 & 0.0202689 & 0.0069890 & 0.0047986 & -0.0023232 & 0.0028356 & 0.0044204 & -0.0213801 & 0.0052154 & 0.0071755 & 0.0055331 & 0.0050821 & 0.0036197 & 0.0013914 & 0.0033442 & 0.0056231 & -0.0252218 & -0.0117840 & 0.0061928 & -0.0106124 & 0.0049503 & 0.0034818\\\\\n0.0003548 & -0.0353656 & -83.959288 & 81.0229845 & -0.0323793 & -0.0125648 & -0.0100731 & -0.0217221 & -0.0009956 & -0.0052225 & -0.0077043 & -0.0035673 & -0.0015516 & -0.0412529 & -0.0072744 & -0.0103216 & -0.0175768 & -0.0061291 & -0.0055781 & -0.0250115 & -0.0031487 & -0.0024306 & 0.0259107 & 0.0020011 & -0.0096025 & -0.0020035 & -0.0075789 & -0.0038730 & -0.0199645 & -0.0025073 & -0.0018426 & 0.0519454 & 0.0279126 & -0.0113689 & 0.0239012 & -0.0041041 & -0.0027491\\\\\n0.0003375 & -0.0856993 & -17.668229 & 12.6951078 & 1.5369754 & 0.0157813 & 0.0025879 & 0.0101841 & 0.0064898 & 0.0035913 & -0.0026825 & 0.0018438 & 0.0023374 & -0.0498738 & -0.0053213 & 0.0027397 & -0.0060724 & 0.0036489 & 0.0014942 & 0.0048228 & 0.0031182 & 0.0014520 & -0.0370474 & -0.0105046 & 0.0021995 & -0.0117505 & 0.0000522 & -0.0001389 & -0.0065445 & 0.0035055 & 0.0041521 & -0.0170625 & 0.0019989 & 0.0021959 & 0.0015411 & -0.0000580 & 0.0011796\\\\\n0.0016069 & -0.0340099 & 19.401443 & -15.3703471 & -0.5493958 & 2.6224419 & 0.0003326 & 0.0010981 & 0.0083241 & 0.0041466 & 0.0235442 & 0.0018294 & 0.0042138 & -0.0091644 & -0.0023618 & 0.0000538 & -0.0146809 & 0.0069225 & 0.0032846 & 0.0192917 & 0.0020436 & 0.0046468 & -0.0198602 & -0.0110752 & 0.0007751 & -0.0231230 & 0.0050973 & 0.0013701 & -0.0174437 & 0.0025197 & 0.0049645 & 0.0013264 & -0.0037554 & -0.0000235 & -0.0107301 & 0.0036918 & 0.0001874\\\\\n0.0019618 & -0.1610342 & -9.991517 & 9.7535485 & -0.0350005 & 1.2345109 & 40.9682050 & -0.0049999 & 0.0045818 & 0.0085940 & 0.0160180 & 0.0130868 & 0.0119625 & -0.0096225 & -0.0224200 & 0.0166570 & -0.0093221 & 0.0028189 & 0.0083136 & 0.0260148 & 0.0128153 & 0.0115131 & 0.0020661 & -0.0094318 & 0.0155655 & 0.0007142 & 0.0037929 & 0.0082308 & 0.0262800 & 0.0132429 & 0.0114991 & -0.0057478 & -0.0134590 & 0.0151920 & -0.0012691 & 0.0046873 & 0.0069588\\\\\n0.0004096 & 0.5016865 & 12.326654 & -13.8264116 & -0.2662471 & -15.5149013 & 0.7373761 & 16.4581308 & -0.0016515 & 0.0034182 & -0.0071657 & 0.0035049 & 0.0027027 & -0.0069051 & 0.0000421 & 0.0065186 & 0.0022661 & -0.0013775 & 0.0034178 & 0.0247099 & 0.0031323 & 0.0016169 & 0.0018412 & 0.0100142 & 0.0047411 & 0.0123967 & -0.0003545 & 0.0036853 & 0.0300402 & 0.0036418 & 0.0013277 & -0.0047946 & 0.0094670 & 0.0069562 & 0.0087030 & 0.0016602 & 0.0042648\\\\\n0.0009858 & -0.2239827 & 64.074187 & -37.6388463 & -0.1242079 & -1.3331271 & -6.9042185 & 1.2938291 & 24.0535568 & 0.0003073 & -0.0001033 & 0.0045808 & 0.0030591 & -0.0090343 & -0.0109214 & 0.0073192 & -0.0071893 & -0.0053059 & 0.0009555 & 0.0132884 & 0.0064131 & 0.0052017 & 0.0071039 & 0.0134822 & 0.0050019 & 0.0167140 & -0.0061722 & 0.0018474 & 0.0262071 & 0.0066638 & 0.0066726 & 0.0351818 & 0.0282019 & 0.0034235 & 0.0284810 & -0.0039385 & 0.0013751\\\\\n0.0003987 & -0.3226887 & 3.507338 & -14.2262892 & -0.6077691 & -1.1745914 & -15.4361443 & 0.6631769 & -4.7504949 & 61.0261409 & -0.0019005 & 0.0043892 & 0.0024146 & 0.0022355 & -0.0143324 & 0.0091323 & -0.0114421 & 0.0069643 & 0.0012324 & -0.0006509 & 0.0049690 & 0.0026333 & 0.0004229 & 0.0017178 & 0.0096012 & 0.0050086 & 0.0031540 & -0.0004469 & 0.0074498 & 0.0048061 & 0.0007665 & 0.0255526 & 0.0152672 & 0.0070770 & 0.0167176 & -0.0012217 & 0.0001494\\\\\n\\end{array}\n\\right)\\, .\n\\end{equation*}\\]\next.data_kappa0 = pmatrix(ext.struct.means$kappa0)\n\\[\\begin{equation*}\n\\left(\n\\begin{array}{c}\nx\\\\\n\\hline\n9936.088\\\\\n\\end{array}\n\\right)\\, .\n\\end{equation*}\\]\next.data_kappap = pmatrix(ext.struct.means$kappap)\n\\[\\begin{equation*}\n\\left(\n\\begin{array}{c}\nx\\\\\n\\hline\n0.0002731\\\\\n\\end{array}\n\\right)\\, .\n\\end{equation*}\\]"
  },
  {
    "objectID": "index.html#baseline-model",
    "href": "index.html#baseline-model",
    "title": "Estimating the impact of monetary policy shocks on different housing indicators: A Bayesian SVAR Approach",
    "section": "Baseline Model",
    "text": "Baseline Model\n\n\n\n\n\n\n\n\n\n\nImpulse Response Functions (IRFs)\n\n\n\n\n\n\n\n\nIRF plots for the AU Cash Rate Shock\n\n\n\n\n\n\n\n\n\nIRF plots for the US Federal Funds Rate Shock\n\n\n\n\n\n\n\n\n\nForecast Error Variance Decomposition"
  },
  {
    "objectID": "index.html#extended-model-1",
    "href": "index.html#extended-model-1",
    "title": "Estimating the impact of monetary policy shocks on different housing indicators: A Bayesian SVAR Approach",
    "section": "Extended Model",
    "text": "Extended Model\n\n\n\n\n\n\n\n\n\n\nImpulse response functions\n\n\n\n\n\nIRF plots for the AU Cash Rate Shock, Extended Model\n\n\n\n\n\n\n\n\n\nIRF plots for the US Federal Funds Rate Shock, Extended Model\n\n\n\n\n\n\nForecast Error Variance Decomposition"
  }
]